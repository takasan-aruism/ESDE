# PROJECT SIGNATURE SKELETON (Copy this to new AI session)
# This defines strictly exist classes and methods.

# FILE: engine.py
class ESDEEngine:
    '''ESDE Engine with Phase 7A+ Multi-Hypothesis Routing.'''
    def __init__(self, synapse_file: str, glossary_file: str, max_synsets_per_token: int, top_concepts: int, axis_top_levels: int, dedup_per_token: bool, enable_proximity: bool, max_proxy_depth: int, debug_proximity: bool, queue_path: str, queue_include_noise: bool): ...
    def initialize(self): ...
    def _get_config_meta(self): ...
    def process(self, text: str, include_evidence: bool): ...
    def process_batch(self, texts: List[str]): ...

----------------------------------------
# FILE: hypothesis.py
class HypothesisResult:
    '''Result for a single hypothesis (A/B/C/D).'''
    def confidence(self): ...
    def to_dict(self): ...

class EvaluationReport:
    '''Full evaluation report for a token.'''
    def __post_init__(self): ...
    def _check_consistency(self): ...
    def to_dict(self): ...
    def format_scores(self): ...
    def format_log_line(self): ...
    def get_strongest_hypothesis(self): ...

class ScoringDecision:
    '''Legacy format for backward compatibility.'''
    def to_dict(self): ...

def evaluate_hypothesis_a(token: str, evidence: EvidenceCollection, typo_candidates: List[Dict], queue_metrics: Dict[str, float]): ...

def evaluate_hypothesis_b(token: str, evidence: EvidenceCollection, queue_metrics: Dict[str, float]): ...

def evaluate_hypothesis_c(token: str, evidence: EvidenceCollection, queue_metrics: Dict[str, float]): ...

def evaluate_hypothesis_d(token: str, evidence: EvidenceCollection, queue_metrics: Dict[str, float]): ...

def compute_competing_info(hypotheses: Dict[str, HypothesisResult]): ...

def compute_competing_count(hypotheses: Dict[str, HypothesisResult]): ...

def compute_sense_conflict(hypotheses: Dict[str, HypothesisResult]): ...

def compute_source_conflict(evidence: EvidenceCollection): ...

def compute_evidence_dispersion(evidence: EvidenceCollection): ...

def compute_global_volatility(hypotheses: Dict[str, HypothesisResult], evidence: EvidenceCollection): ...

def determine_status(global_volatility: float, competing_count: int): ...

def evaluate_all_hypotheses(token: str, evidence: EvidenceCollection, typo_candidates: List[Dict], queue_metrics: Dict[str, float]): ...

def score_all_hypotheses(token: str, evidence: EvidenceCollection, typo_candidates: List[Dict], queue_metrics: Dict[str, float]): ...

def determine_action(decision: ScoringDecision, conf_threshold: float, vol_threshold: float, quarantine_on_vol: bool): ...

----------------------------------------
# FILE: online.py
def get_source_tier(url: str): ...

class SearchQuery:
    '''A search query with metadata.'''
    pass

def generate_queries(token: str, context: Dict[str, Any], typo_candidates: List[Dict], route_winner: str): ...

def detect_title_phrase(token: str, context: Dict[str, Any]): ...

class EvidenceItem:
    '''A piece of evidence extracted from a search result.'''
    def to_dict(self): ...

class EvidenceCollection:
    '''Collection of evidence for a token.'''
    def add(self, item: EvidenceItem): ...
    def to_dict(self): ...

def extract_signals(title: str, snippet: str): ...

def extract_claims(token: str, title: str, snippet: str, signals: Dict[str, bool]): ...

def create_evidence_item(url: str, title: str, snippet: str, token: str): ...

class SearchProvider:
    '''Abstract interface for search providers.'''
    def search(self, query: str, max_results: int): ...

class MockSearchProvider(SearchProvider):
    '''Mock search provider for testing.'''
    def __init__(self): ...
    def search(self, query: str, max_results: int): ...

def collect_evidence(token: str, context: Dict[str, Any], search_provider: SearchProvider, typo_candidates: List[Dict], route_winner: str, min_sources: int, max_sources: int, cache): ...

----------------------------------------
# FILE: aggregate_state.py
class AggregateStateManager:
    '''Manages state at aggregate_key level for Phase 7B+.'''
    def __init__(self, state_file: str): ...
    def _ensure_directory(self): ...
    def compute_aggregate_key(token_norm: str, pos: str, route_set: Set[str]): ...
    def load(self): ...
    def save(self): ...
    def get_entry(self, aggregate_key: str): ...
    def should_process(self, aggregate_key: str): ...
    def upsert_observation(self, aggregate_key: str, token_norm: str, pos: str, route_set: Set[str], evaluation_result: Dict[str, Any], run_id: str): ...
    def _compute_needs_observation(self, entry: Dict[str, Any]): ...
    def mark_finalized(self, aggregate_key: str, reason: str): ...
    def reset_for_reprocess(self, include_finalized: bool): ...
    def get_stats(self): ...
    def get_pending_aggregates(self, limit: int): ...

----------------------------------------
# FILE: esde_sensor.py
class ESDESensor:
    '''Semantic Operators v0.1-lean 統合センサー'''
    def __init__(self, engine: ESDEngine, base_url, api_key, model): ...
    def _build_trigger_index(self): ...
    def _build_anti_trigger_index(self): ...
    def _build_name_index(self): ...
    def _build_axes_info(self): ...
    def _build_formula(self, text: str, extracted_concept: str): ...
    def _resolve_formula(self, formula: Dict): ...
    def _formula_to_string(self, formula: Dict): ...
    def _simple_stem(self, word: str): ...
    def _extract_candidates(self, text: str): ...
    def _detect_negation(self, text: str): ...
    def _build_candidate_prompt(self, candidates: List[str]): ...
    def _build_fallback_prompt(self): ...
    def _call_llm_select(self, text: str, candidates: List[str]): ...
    def _call_llm_fallback(self, text: str): ...
    def _call_llm_api(self, prompt: str): ...
    def _extract_json(self, text: str): ...
    def analyze(self, text: str): ...
    def _guess_axis(self, concept_id: str): ...
    def _guess_level(self, concept_id: str): ...
    def _validate(self, data: Dict): ...
    def analyze_and_observe(self, text: str): ...
    def _print_report(self, r, formula): ...
    def observe(self, text: str): ...
    def validate(self, data: Dict): ...

----------------------------------------
# FILE: generate_synapses_v2_1.py
def ensure_nltk_data(): ...

def safe_load_esde(path: str): ...

def softmax(scores: list, temp: float): ...

def select_topk_with_diversity(candidates: list, topk: int, max_per_concept: int): ...

def main(): ...

----------------------------------------
# FILE: resolve_unknown_queue_7bplus_v534_final.py
def get_route_set_from_record(record: Dict[str, Any]): ...

class AuditLedger:
    '''Append-only audit ledger for 7B+ evaluations.'''
    def __init__(self, ledger_file: str): ...
    def append(self, record: Dict[str, Any]): ...

class AggregatedRecord:
    '''Aggregated multi-hypothesis record for output.'''
    def __init__(self, token_norm: str, pos: str, route_set: Set[str]): ...
    def add_occurrence(self, report: EvaluationReport, original_record: Dict[str, Any], run_id: str): ...
    def get_final_status(self): ...
    def to_dict(self): ...

def load_queue_records(queue_file: str): ...

def filter_candidates(records: List[Dict[str, Any]], agg_state: AggregateStateManager, include_finalized: bool): ...

def evaluate_token(record: Dict[str, Any], search_provider, cache: SearchCache, min_sources: int, max_sources: int): ...

def process_batch(records: List[Dict[str, Any]], agg_state: AggregateStateManager, legacy_state: QueueStateManager, audit_ledger: AuditLedger, search_provider, cache: SearchCache, limit: int, min_sources: int, max_sources: int, dry_run: bool, reprocess: bool, run_id: str): ...

def audit_structure(aggregate: Dict[str, Any], state_entry: Dict[str, Any]): ...

def extract_hypothesis_snapshot(aggregate: Dict[str, Any]): ...

def extract_volatility_snapshot(aggregate: Dict[str, Any]): ...

def generate_narrative(aggregate: Dict[str, Any]): ...

def run_7c_audit(aggregate: Dict[str, Any], state_entry: Dict[str, Any]): ...

def load_aggregated_queue(file_path: str): ...

def append_audit_log(record: Dict[str, Any], file_path: str): ...

def run_7c_mode(state_file: str, aggregated_file: str, audit_log_file: str): ...

def show_stats(agg_state: AggregateStateManager, cache: SearchCache): ...

def main(): ...

----------------------------------------